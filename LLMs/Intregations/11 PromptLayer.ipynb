{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e2c581-11e8-4034-901f-9a0427902bb8",
   "metadata": {},
   "source": [
    "### This allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879d1c8f-7e6d-4450-8742-b37fea178a40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting promptlayer\n",
      "  Downloading promptlayer-0.1.80.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from promptlayer) (2.28.2)\n",
      "Requirement already satisfied: openai in c:\\python311\\lib\\site-packages (from promptlayer) (0.27.2)\n",
      "Requirement already satisfied: langchain in c:\\python311\\lib\\site-packages (from promptlayer) (0.0.123)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (1.24.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (1.10.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python311\\lib\\site-packages (from langchain->promptlayer) (8.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->promptlayer) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->promptlayer) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->promptlayer) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->promptlayer) (2022.12.7)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from openai->promptlayer) (4.64.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->promptlayer) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->promptlayer) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->promptlayer) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->promptlayer) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python311\\lib\\site-packages (from pydantic<2,>=1->langchain->promptlayer) (4.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python311\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain->promptlayer) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm->openai->promptlayer) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\python311\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain->promptlayer) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python311\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->promptlayer) (1.0.0)\n",
      "Installing collected packages: promptlayer\n",
      "  Running setup.py install for promptlayer: started\n",
      "  Running setup.py install for promptlayer: finished with status 'done'\n",
      "Successfully installed promptlayer-0.1.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: promptlayer is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n"
     ]
    }
   ],
   "source": [
    "!pip install promptlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d0dca9-0e77-4f59-b553-a742fe117768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import PromptLayerOpenAI\n",
    "import promptlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50149a09-95d3-4d74-8860-3f84a0f74b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your key ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "API_KEY=getpass(\"Enter your key\")\n",
    "\n",
    "os.environ['PROMPTLAYER_API_KEY']=API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac79a0b1-6eba-4c88-bdba-31e4e7049e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# OPENAI_API_KEY=getpass(\"Enter your key\")\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dbeb77d-ded8-467c-8e88-0e7fcfde5b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please set your PROMPTLAYER_API_KEY environment variable or set API KEY in code using 'promptlayer.api_key = <your_api_key>' ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m PromptLayerOpenAI(pl_tags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI am a cat and I want\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\langchain\\llms\\promptlayer_openai.py:62\u001b[0m, in \u001b[0;36mPromptLayerOpenAI._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m     48\u001b[0m generation \u001b[38;5;241m=\u001b[39m generated_responses\u001b[38;5;241m.\u001b[39mgenerations[i][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m resp \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: generated_responses\u001b[38;5;241m.\u001b[39mllm_output,\n\u001b[0;32m     52\u001b[0m }\n\u001b[0;32m     53\u001b[0m pl_request_id \u001b[38;5;241m=\u001b[39m promptlayer_api_request(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain.PromptLayerOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m     [prompt],\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identifying_params,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpl_tags,\n\u001b[0;32m     59\u001b[0m     resp,\n\u001b[0;32m     60\u001b[0m     request_start_time,\n\u001b[0;32m     61\u001b[0m     request_end_time,\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mget_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     63\u001b[0m     return_pl_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_pl_id,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_pl_id:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mgeneration_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m     67\u001b[0m         generation\u001b[38;5;241m.\u001b[39mgeneration_info, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m     68\u001b[0m     ):\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\promptlayer\\utils.py:21\u001b[0m, in \u001b[0;36mget_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_api_key\u001b[39m():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# raise an error if the api key is not set\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m promptlayer\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set your PROMPTLAYER_API_KEY environment variable or set API KEY in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpromptlayer.api_key = <your_api_key>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m         )\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m promptlayer\u001b[38;5;241m.\u001b[39mapi_key\n",
      "\u001b[1;31mException\u001b[0m: Please set your PROMPTLAYER_API_KEY environment variable or set API KEY in code using 'promptlayer.api_key = <your_api_key>' "
     ]
    }
   ],
   "source": [
    "llm = PromptLayerOpenAI(pl_tags=[\"langchain\"])\n",
    "llm(\"I am a cat and I want\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
