{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8131d17-1dae-4235-bdf7-70624ba31cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\python311\\lib\\site-packages (0.0.123)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\python311\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\python311\\lib\\site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python311\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\python311\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python311\\lib\\site-packages (from langchain) (1.24.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\python311\\lib\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python311\\lib\\site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python311\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python311\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python311\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\python311\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python311\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: OpenAI in c:\\python311\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\python311\\lib\\site-packages (from OpenAI) (2.28.2)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from OpenAI) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\python311\\lib\\site-packages (from OpenAI) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.20->OpenAI) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.20->OpenAI) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.20->OpenAI) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.20->OpenAI) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp->OpenAI) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm->OpenAI) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f997388-756f-4115-9c27-38d269d499c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Key ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "api_key=getpass(\"Enter your Key\")\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebde9ade-4a6f-4181-a4c8-d0ff61ce7f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm=OpenAI(model_name=\"text-davinci-003\",max_tokens=\"2000\")\n",
    "no_cache_llm=OpenAI(model_name=\"text-davinci-003\",cache=False,max_tokens=\"2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b1b974d-602c-4fe6-a63a-2888b9ceab93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CharacterTextSplitter in module langchain.text_splitter object:\n",
      "\n",
      "class CharacterTextSplitter(TextSplitter)\n",
      " |  CharacterTextSplitter(separator: 'str' = '\\n\\n', **kwargs: 'Any')\n",
      " |  \n",
      " |  Implementation of splitting text that looks at characters.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CharacterTextSplitter\n",
      " |      TextSplitter\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, separator: 'str' = '\\n\\n', **kwargs: 'Any')\n",
      " |      Create a new TextSplitter.\n",
      " |  \n",
      " |  split_text(self, text: 'str') -> 'List[str]'\n",
      " |      Split incoming text and return chunks.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from TextSplitter:\n",
      " |  \n",
      " |  create_documents(self, texts: 'List[str]', metadatas: 'Optional[List[dict]]' = None) -> 'List[Document]'\n",
      " |      Create documents from a list of texts.\n",
      " |  \n",
      " |  split_documents(self, documents: 'List[Document]') -> 'List[Document]'\n",
      " |      Split documents.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from TextSplitter:\n",
      " |  \n",
      " |  from_huggingface_tokenizer(tokenizer: 'Any', **kwargs: 'Any') -> 'TextSplitter' from abc.ABCMeta\n",
      " |      Text splitter that uses HuggingFace tokenizer to count length.\n",
      " |  \n",
      " |  from_tiktoken_encoder(encoding_name: 'str' = 'gpt2', allowed_special: \"Union[Literal['all'], AbstractSet[str]]\" = set(), disallowed_special: \"Union[Literal['all'], Collection[str]]\" = 'all', **kwargs: 'Any') -> 'TextSplitter' from abc.ABCMeta\n",
      " |      Text splitter that uses tiktoken encoder to count length.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from TextSplitter:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "\n",
    "text_splitter= CharacterTextSplitter()\n",
    "\n",
    "help(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ebfb82d-b424-41c8-af46-401469f3c0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are deeply alarmed by the relentless lawfare (legal warfare) being waged against press freedom and the freedom of speech using various legal instruments. The latest victims of this are a reporter and the editor of Prothom Alo, who have been sued under the Digital Security Act (DSA) for \"tarnishing the country\\'s image\" using its platforms. The hint, of course, was to the daily\\'s May 26 publication of a report on the astronomical prices of food using a comment from a day labourer, which has since gone viral. The question is, how does the coverage of poor people\\'s struggles constitute a crime? This is nothing but an attempt to intimidate journalists and discredit the message so powerfully captured in said comment.\\n\\nProthom Alo journalist arrest\\nRead more\\nPersecution of journalists: Condemnation isn’t enough\\nReportedly, the general secretary of the ruling party called the report \"politically motivated\". In all fairness, it is the case filed in this connection that seems politically motivated. The report was done following standard journalistic practices, and Prothom Alo was quick to take appropriate action after an apparent inconsistency was pointed out in its social media post. That should have been the end of it. Anyone with the remotest idea of how the news media works would know that the reporter, Samsuzzaman Shams, was innocent of the \"crimes\" alleged by pro-AL factions. Yet, he was picked up from his house by officers of the Criminal Investigation Department (CID). His whereabouts were kept hidden for about 20 hours. And two cases were filed against him before he was finally shown arrested in one and produced before a court.\\n\\nFor all latest news, follow The Daily Star\\'s Google News channel.\\nProthom Alo reporter Samsuzzaman Shams Arrested\\nVictimising Shams shows why DSA should go\\nThis elaborate charade to harass him reminds us of the legal mumbo jumbo in which another Prothom Alo reporter, Rozina Islam, was implicated not long ago. On Thursday afternoon, drawing a 35-hour-long saga to an end, Shams was sent to jail after being denied bail by the court. On Wednesday, a journalist of Jugantor was sued under the DSA for reporting on a railway syndicate operating in Chattogram. Earlier, the brother of a Bangladeshi journalist of Al Jazeera was allegedly beaten with iron rods for \"writing about the PM\". In mid-March, journalists covering the election of the Supreme Court Bar Association were assaulted by police. The frequency with which journalists are coming under attack, legally or physically, is really disturbing.\\n\\nAwami League dragging its feet over promised reforms to the Digital Security Act\\nRead more\\nAwami League wants us to love DSA. It’s too late now\\nThe DSA, for obvious reasons, has emerged as the most preferred tool of repression of journalists and critical voices, as a review by the Centre for Governance Studies (CGS) has shown. The judiciary\\'s role in this regard has also been wanting. We expect the judiciary to stand in favour of journalists especially when they are harassed with trumped-up charges. Unfortunately, just like DSA, the entire legal infrastructure is being instrumentalised today to \"chill critical reporting\", undermine press freedom, and encourage a culture of self-censorship. This is no doubt an affront to everything that we stand for as a nation. The question is, how long will the authorities ignore calls to create an environment in which the press can function without any fear of consequences?\\n\\nRozina Akhter legal harassment \\nRead more\\nDrop the charges against Rozina Islam\\nShams doesn\\'t deserve to be in jail. The Prothom Alo editor, Matiur Rahman, doesn\\'t deserve to have a DSA case filed against him. All such cases do not deserve to be entertained. We hope that the judiciary will stand with the press and take appropriate steps in this regard soon.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./en_news.txt\",encoding=\"utf-8\") as f:\n",
    "    news=f.read()\n",
    "texts= text_splitter.split_text(news)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3dfb7-f49e-474d-80ec-21a7cfec28bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca1fdfc5-7d5f-4fb7-bfa1-3e47160bf70c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We are deeply alarmed by the relentless lawfare (legal warfare) being waged against press freedom and the freedom of speech using various legal instruments. The latest victims of this are a reporter and the editor of Prothom Alo, who have been sued under the Digital Security Act (DSA) for \"tarnishing the country\\'s image\" using its platforms. The hint, of course, was to the daily\\'s May 26 publication of a report on the astronomical prices of food using a comment from a day labourer, which has since gone viral. The question is, how does the coverage of poor people\\'s struggles constitute a crime? This is nothing but an attempt to intimidate journalists and discredit the message so powerfully captured in said comment.\\n\\nProthom Alo journalist arrest\\nRead more\\nPersecution of journalists: Condemnation isn’t enough\\nReportedly, the general secretary of the ruling party called the report \"politically motivated\". In all fairness, it is the case filed in this connection that seems politically motivated. The report was done following standard journalistic practices, and Prothom Alo was quick to take appropriate action after an apparent inconsistency was pointed out in its social media post. That should have been the end of it. Anyone with the remotest idea of how the news media works would know that the reporter, Samsuzzaman Shams, was innocent of the \"crimes\" alleged by pro-AL factions. Yet, he was picked up from his house by officers of the Criminal Investigation Department (CID). His whereabouts were kept hidden for about 20 hours. And two cases were filed against him before he was finally shown arrested in one and produced before a court.\\n\\nFor all latest news, follow The Daily Star\\'s Google News channel.\\nProthom Alo reporter Samsuzzaman Shams Arrested\\nVictimising Shams shows why DSA should go\\nThis elaborate charade to harass him reminds us of the legal mumbo jumbo in which another Prothom Alo reporter, Rozina Islam, was implicated not long ago. On Thursday afternoon, drawing a 35-hour-long saga to an end, Shams was sent to jail after being denied bail by the court. On Wednesday, a journalist of Jugantor was sued under the DSA for reporting on a railway syndicate operating in Chattogram. Earlier, the brother of a Bangladeshi journalist of Al Jazeera was allegedly beaten with iron rods for \"writing about the PM\". In mid-March, journalists covering the election of the Supreme Court Bar Association were assaulted by police. The frequency with which journalists are coming under attack, legally or physically, is really disturbing.\\n\\nAwami League dragging its feet over promised reforms to the Digital Security Act\\nRead more\\nAwami League wants us to love DSA. It’s too late now\\nThe DSA, for obvious reasons, has emerged as the most preferred tool of repression of journalists and critical voices, as a review by the Centre for Governance Studies (CGS) has shown. The judiciary\\'s role in this regard has also been wanting. We expect the judiciary to stand in favour of journalists especially when they are harassed with trumped-up charges. Unfortunately, just like DSA, the entire legal infrastructure is being instrumentalised today to \"chill critical reporting\", undermine press freedom, and encourage a culture of self-censorship. This is no doubt an affront to everything that we stand for as a nation. The question is, how long will the authorities ignore calls to create an environment in which the press can function without any fear of consequences?\\n\\nRozina Akhter legal harassment \\nRead more\\nDrop the charges against Rozina Islam\\nShams doesn\\'t deserve to be in jail. The Prothom Alo editor, Matiur Rahman, doesn\\'t deserve to have a DSA case filed against him. All such cases do not deserve to be entertained. We hope that the judiciary will stand with the press and take appropriate steps in this regard soon.', lookup_str='', metadata={}, lookup_index=0)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "# help(Document)\n",
    "# for i, t in enumerate(texts[:3]):\n",
    "#     doc=Document(page_content=t)\n",
    "#     print(doc)\n",
    "docs= [Document(page_content=text) for text in texts[:3]]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42f2f238-89b5-4a30-ad3b-f6ba665a7319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2fbf529a-cf4c-45d3-86ac-8cd48c2518e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MapReduceDocumentsChain in module langchain.chains.combine_documents.map_reduce object:\n",
      "\n",
      "class MapReduceDocumentsChain(langchain.chains.combine_documents.base.BaseCombineDocumentsChain, pydantic.main.BaseModel)\n",
      " |  MapReduceDocumentsChain(*, memory: Optional[langchain.schema.BaseMemory] = None, callback_manager: langchain.callbacks.base.BaseCallbackManager = None, verbose: bool = None, input_key: str = 'input_documents', output_key: str = 'output_text', llm_chain: langchain.chains.llm.LLMChain, combine_document_chain: langchain.chains.combine_documents.base.BaseCombineDocumentsChain, collapse_document_chain: Optional[langchain.chains.combine_documents.base.BaseCombineDocumentsChain] = None, document_variable_name: str, return_intermediate_steps: bool = False) -> None\n",
      " |  \n",
      " |  Combining documents by mapping a chain over them, then combining results.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MapReduceDocumentsChain\n",
      " |      langchain.chains.combine_documents.base.BaseCombineDocumentsChain\n",
      " |      langchain.chains.base.Chain\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async acombine_docs(self, docs: 'List[Document]', **kwargs: 'Any') -> 'Tuple[str, dict]'\n",
      " |      Combine documents in a map reduce manner.\n",
      " |      \n",
      " |      Combine by mapping first chain over all documents, then reducing the results.\n",
      " |      This reducing can be done recursively if needed (if there are many documents).\n",
      " |  \n",
      " |  combine_docs(self, docs: 'List[Document]', token_max: 'int' = 3000, **kwargs: 'Any') -> 'Tuple[str, dict]'\n",
      " |      Combine documents in a map reduce manner.\n",
      " |      \n",
      " |      Combine by mapping first chain over all documents, then reducing the results.\n",
      " |      This reducing can be done recursively if needed (if there are many documents).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_default_document_variable_name(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      Get default document variable name, if not provided.\n",
      " |  \n",
      " |  get_return_intermediate_steps(values: 'Dict') -> 'Dict' from pydantic.main.ModelMetaclass\n",
      " |      For backwards compatibility.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  output_keys\n",
      " |      Expect input key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain.chains.combine_documents.map_reduce.MapRedu...\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'collapse_document_chain': 'Optional[BaseCombineDoc...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'langchain.chains.combine_documents.map_reduce.Con...\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True}\n",
      " |  \n",
      " |  __fields__ = {'callback_manager': ModelField(name='callback_manager', ...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function MapReduceDocumentsChain.get_retur...\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, memory: Optional[langchain.schema...ret...\n",
      " |  \n",
      " |  __validators__ = {'callback_manager': [<pydantic.class_validators.Vali...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.combine_documents.base.BaseCombineDocumentsChain:\n",
      " |  \n",
      " |  prompt_length(self, docs: List[langchain.schema.Document], **kwargs: Any) -> Optional[int]\n",
      " |      Return the prompt length given the documents passed in.\n",
      " |      \n",
      " |      Returns None if the method does not depend on the prompt length.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.chains.combine_documents.base.BaseCombineDocumentsChain:\n",
      " |  \n",
      " |  input_keys\n",
      " |      Expect input key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __call__(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False) -> Dict[str, Any]\n",
      " |      Run the logic of this chain and add to output if desired.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param.\n",
      " |          return_only_outputs: boolean for whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |  \n",
      " |  async acall(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False) -> Dict[str, Any]\n",
      " |      Run the logic of this chain and add to output if desired.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param.\n",
      " |          return_only_outputs: boolean for whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |  \n",
      " |  apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]\n",
      " |      Call the chain on all inputs in the list.\n",
      " |  \n",
      " |  async arun(self, *args: str, **kwargs: str) -> str\n",
      " |      Run the chain as text in, text out or multiple variables, text out.\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Return dictionary representation of chain.\n",
      " |  \n",
      " |  prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]\n",
      " |      Validate and prep inputs.\n",
      " |  \n",
      " |  prep_outputs(self, inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) -> Dict[str, str]\n",
      " |      Validate and prep outputs.\n",
      " |  \n",
      " |  run(self, *args: str, **kwargs: str) -> str\n",
      " |      Run the chain as text in, text out or multiple variables, text out.\n",
      " |  \n",
      " |  save(self, file_path: Union[pathlib.Path, str]) -> None\n",
      " |      Save the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the chain to.\n",
      " |      \n",
      " |      Example:\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          chain.save(file_path=\"path/chain.yaml\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  set_callback_manager(callback_manager: Optional[langchain.callbacks.base.BaseCallbackManager]) -> langchain.callbacks.base.BaseCallbackManager from pydantic.main.ModelMetaclass\n",
      " |      If callback manager is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as callback manager, which is a nice UX.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic.main.ModelMetaclass\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain= load_summarize_chain(llm,chain_type='map_reduce',reduce_llm=no_cache_llm)\n",
    "help(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "50ff49ab-e4bb-484c-8dbd-8bf9ee6077ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Journalists in Bangladesh are facing increasing legal and physical attacks for their work, with a recent example of a Prothom Alo reporter being arrested and charged for his coverage of food prices. The judiciary has been complicit in this, and calls to reform the Digital Security Act have been ignored. This is a clear infringement of press freedom, and charges against journalists should be dropped.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
